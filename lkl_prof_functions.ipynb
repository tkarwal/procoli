{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2673a488-fb5b-458d-98b6-6fc1812876f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getdist import mcsamples, plots, chains\n",
    "from getdist.mcsamples import MCSamplesError\n",
    "import numpy as np\n",
    "from subprocess import run\n",
    "import os\n",
    "from yaml import dump\n",
    "from cobaya.yaml import yaml_load_file\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f1156-2ff3-41eb-bb9e-9531a94f9448",
   "metadata": {},
   "source": [
    "### Variables used for functions and testing them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b2c47-7892-4541-8b84-e15b7ba4e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chains_dir = \"/Users/tanvikarwal/Desktop/Early_dark_energy/likelihood_profile/chains/lcdm_base/\"\n",
    "# chain_file = 'lcdm_cmb_bao_sne_'\n",
    "# chains_dir = '/home2/karwal/mcmc_chains/ede_lkl_profile/lcdm_base/'\n",
    "# chain_file = 'lcdm_cmb_bao_sne_'\n",
    "# os.chdir(chains_dir)\n",
    "\n",
    "# settings = {'ignore_rows' : 0.2}\n",
    "# mcmc_chains = mcsamples.loadMCSamples(chains_dir+chain_file, settings=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c20199-5b16-438e-910d-ff66815b4860",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read minimum file and save parameter names and values lists and MLs dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca0980-e4c4-4fa4-a605-8118e8938185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_minimum(chains_dir, chain_file, prof_incr, extension='_lkl_prof'):\n",
    "    if len(extension) != 0:\n",
    "        if prof_incr > 0:\n",
    "            extension = '_p'+extension\n",
    "        if prof_incr < 0:\n",
    "            extension = '_n'+extension\n",
    "    param_ML, param_names = np.loadtxt(chains_dir + chain_file + extension + '.minimum', skiprows=3, usecols = (1,2), dtype=str, unpack=True)\n",
    "    param_ML = param_ML.astype(float)\n",
    "    with open(chains_dir + chain_file + extension + '.minimum') as min_file:\n",
    "        loglkl_and_chi = [next(min_file) for x in range(2)] # reading in the first two lines separately for -log(lkl) and chi^2\n",
    "    for line in loglkl_and_chi:\n",
    "        param_names = np.append(param_names, line.split(\"=\")[0])\n",
    "        param_ML = np.append(param_ML, float(line.split(\"=\")[1]))\n",
    "    MLs = dict(zip(param_names, param_ML))\n",
    "    return param_names, param_ML, MLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a97e67-e738-4b4e-838b-8c7bac6a3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_names, param_values, MLs = read_minimum(chains_dir=chains_dir, chain_file=chain_file)\n",
    "# MLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e77456-86fc-41c3-8557-b3ea929685b6",
   "metadata": {},
   "source": [
    "### Read last line of lkl prof output file into list and update MLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e429a82-9041-4a77-bf44-d76c8325c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lkl_output(chains_dir, chain_file, prof_incr, extension='_lkl_profile.txt', loc=-1):\n",
    "    if len(extension) != 0:\n",
    "        if prof_incr > 0:\n",
    "            extension = '_p'+extension\n",
    "        if prof_incr < 0:\n",
    "            extension = '_n'+extension\n",
    "    lkl_prof_table = np.loadtxt(chains_dir + chain_file + extension)\n",
    "    try:\n",
    "        lkl_prof_table.shape[1] # check that lkl_prof_table has multiple rows\n",
    "        lkl_prof_table = lkl_prof_table[-1, :]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    return lkl_prof_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59689ce8-253e-4a37-a3bb-5080fd4f1818",
   "metadata": {},
   "source": [
    "### Write params from MLs dict into txt file in append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efca98-9231-40a6-892e-cc76f7b06a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_MLs(param_names, MLs, chains_dir, chain_file, prof_incr, extension='_lkl_profile.txt'):\n",
    "    if len(extension) != 0:\n",
    "        if prof_incr > 0:\n",
    "            extension = '_p'+extension\n",
    "        if prof_incr < 0:\n",
    "            extension = '_n'+extension\n",
    "    with open(chains_dir + chain_file + extension, 'a') as lkl_txt:\n",
    "        for param in param_names:\n",
    "            lkl_txt.write(\"\\t %s\" % str(MLs[param]))\n",
    "        lkl_txt.write(\"\\n\")\n",
    "    lkl_prof_table = np.loadtxt(chains_dir + chain_file + extension)\n",
    "    return lkl_prof_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67fdb4-713c-443f-88e2-8a61e32ecc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_MLs(chains_dir=chains_dir, chain_file=chain_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35d5aa-9008-428d-af02-6e0405c9e552",
   "metadata": {},
   "source": [
    "### Check that param names match in target file and MLs dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c2b0a-0af0-48dc-9b04-2e2494caca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_param_names(param_names, chains_dir, chain_file, prof_incr, extension='_lkl_profile.txt'):\n",
    "    if len(extension) != 0:\n",
    "        if prof_incr > 0:\n",
    "            extension = '_p'+extension\n",
    "        if prof_incr < 0:\n",
    "            extension = '_n'+extension\n",
    "    with open(chains_dir + chain_file + extension, 'r') as lkl_txt:\n",
    "        params_recorded = lkl_txt.readline()\n",
    "    # params_recorded = params_recorded\n",
    "    # define the expected first row of this file\n",
    "    expected_string = '#'\n",
    "    for param in param_names:\n",
    "        expected_string += \"\\t %s\" % param\n",
    "    expected_string += \"\\n\"\n",
    "    if expected_string == params_recorded:\n",
    "        print(\"match_param_names: Found existing file with correct name and parameters / parameter sequence. Will append to it. \\n\" \n",
    "                 + chains_dir + chain_file + extension)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"match_param_names: Error: existing file found at \" + chains_dir + chain_file + extension \n",
    "             + \"\\n but parameters / parameter sequence does not match expected.\")\n",
    "        print(\"--> parameters found: \\n\" + params_recorded)\n",
    "        print(\"--> parameters expected: \\n\" + expected_string)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea35f4a-05da-47e7-92e8-c54e1f50bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_param_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96150242-2810-49ca-95ef-8ce3096f8d52",
   "metadata": {},
   "source": [
    "### Check if some location in lkl_prof output file matches current MLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54d4ac-83dd-4a3d-8217-d53c2b733e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_param_line(param_names, MLs, chains_dir, chain_file, prof_incr, extension='_lkl_profile.txt', loc=-1):\n",
    "    if len(extension) != 0:\n",
    "        if prof_incr > 0:\n",
    "            extension = '_p'+extension\n",
    "        if prof_incr < 0:\n",
    "            extension = '_n'+extension\n",
    "    lkl_prof_table = np.loadtxt(chains_dir + chain_file + extension)\n",
    "    if lkl_prof_table.size==0:\n",
    "        print(\"match_param_line: File empty \")\n",
    "        return False\n",
    "    else: \n",
    "        try:\n",
    "            lkl_prof_table.shape[1] # check that lkl_prof_table has multiple rows\n",
    "            if False in [lkl_prof_table[loc, np.where(param_names == param)] == MLs[param] for param in param_names]:\n",
    "                return False\n",
    "            else:\n",
    "                return True \n",
    "        except IndexError:\n",
    "            print(\"match_param_line: Only one entry in file, checking that entry \")\n",
    "            if False in [lkl_prof_table[np.where(param_names == param)] == MLs[param] for param in param_names]:\n",
    "                return False \n",
    "            else:\n",
    "                return True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9cf44-a0b9-4586-9873-07f7b11829f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_param_line(param_names, MLs, loc=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2410ca4a-f744-4ea0-aeb3-13e0c8316dd3",
   "metadata": {},
   "source": [
    "### Updated yaml info to next increment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b34d0b-476c-495d-8b03-c6f0479eb138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_update_yaml(chains_dir, chain_file, MLs, lkl_pro_yaml, prof_param, prof_incr, yaml_ext = '_lkl_prof'):\n",
    "    if prof_incr > 0:\n",
    "        yaml_ext = '_p'+yaml_ext\n",
    "    if prof_incr < 0:\n",
    "        yaml_ext = '_n'+yaml_ext\n",
    "    # update profile lkl param \n",
    "    latex_info = lkl_pro_yaml['params'][prof_param]['latex']\n",
    "    lkl_pro_yaml['params'][prof_param] = {'value': MLs[prof_param]+prof_incr, 'latex': latex_info}\n",
    "    lkl_pro_yaml['output'] = chain_file + yaml_ext\n",
    "    # update all other independent parameters \n",
    "    for param in lkl_pro_yaml['params']:\n",
    "        if 'prior' in lkl_pro_yaml['params'][param]:\n",
    "            lkl_pro_yaml['params'][param]['ref'] = MLs[param]\n",
    "    # dump yaml to file for running \n",
    "    with open(chains_dir+chain_file+yaml_ext+'.minimize.input.yaml', 'w') as yaml_file:\n",
    "        dump(lkl_pro_yaml, yaml_file, default_flow_style=False)    \n",
    "    return lkl_pro_yaml['params'][prof_param]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a0468-cd92-4cf1-a10e-48cd577f4ee5",
   "metadata": {},
   "source": [
    "### Run minimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287ea52-f1c2-4ead-af67-97f9cdb5d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_minimizer(chain_file, prof_incr, yaml_ext='_lkl_prof', debug=False, processes=6):\n",
    "    \"\"\"\n",
    "    For the parameter we want to vary, remove all but latex and value. \n",
    "    The latex is as before from the MCMC yaml file. \n",
    "    The value is ML $\\pm$ increment. \n",
    "    \"\"\"\n",
    "    if len(yaml_ext) != 0:\n",
    "        if prof_incr > 0:\n",
    "            yaml_ext = '_p'+yaml_ext\n",
    "        if prof_incr < 0:\n",
    "            yaml_ext = '_n'+yaml_ext\n",
    "    if debug==True:\n",
    "        run(\"mpirun -np \"+str(processes)+\" cobaya-run \"+chain_file+yaml_ext+\".minimize.input.yaml -f -d\", shell=True)\n",
    "    else:\n",
    "        run(\"mpirun -np \"+str(processes)+\" cobaya-run \"+chain_file+yaml_ext+\".minimize.input.yaml -f\", shell=True)   \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0d52d1-4052-413b-aaad-a3316324b691",
   "metadata": {},
   "source": [
    "### Check if minimizer was run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5f4e6-8d59-4d55-a3e2-5e0497716047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_global_min(mcmc_chains, chains_dir, chain_file):\n",
    "    try:\n",
    "        mcmc_chains.getParamBestFitDict()\n",
    "        min_yaml = yaml_load_file(chains_dir+chain_file+'.minimize.updated.yaml')\n",
    "        print(\"check_global_min: Found previously run MCMC chains and global minimizer. \")\n",
    "        return True\n",
    "    except MCSamplesError:\n",
    "        print(\"check_global_min: Need to first run a minimizer on the full MCMC chains before beginning 1d profile lkl code.\")\n",
    "        return False \n",
    "    except FileNotFoundError:\n",
    "        print(\"check_global_min: Found best-fit but not the file \"+chains_dir+chain_file+\".minimize.updated.yaml. Something has gone wrong. \")\n",
    "        return FileNotFoundError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd430f-4c8c-4e9e-a229-2df8f11cb537",
   "metadata": {},
   "source": [
    "# Overarching run_prof_lkl function\n",
    "### This should ideally get rewritten after the above code is finalised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712009bf-145f-49fe-b1d3-5a5bd2bdb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_prof_lkl(chains_dir, chain_file, mcmc_chains, processes, minimizer_settings, prof_param, prof_incr, prof_min, prof_max ):\n",
    "#     ### Check if minimizer was run on all chains previously \n",
    "#         # And if not, use input minimizer settings to run it. \n",
    "#         # Ideally, TBC, get number of chains from mcmc and run that many minimizer processes. \n",
    "\n",
    "#     if not check_global_min(mcmc_chains=mcmc_chains, chains_dir=chains_dir, chain_file=chain_file):\n",
    "#         mcmc_yaml = yaml_load_file(chains_dir+chain_file+'.input.yaml')\n",
    "#         mcmc_yaml['sampler'] = minimizer_settings\n",
    "#         min_yaml = deepcopy(mcmc_yaml)\n",
    "#         with open(chains_dir + chain_file + '.minimize.input.yaml', 'w') as yaml_file:\n",
    "#             dump(min_yaml, yaml_file, default_flow_style=False)\n",
    "\n",
    "#         run_minimizer(chain_file=chain_file, yaml_ext='', debug=False, processes=processes)  \n",
    "\n",
    "        \n",
    "#     ### Add minimized point to lkl profile text file \n",
    "#         # So: \n",
    "#         # 1) load the global minimum file. \n",
    "#         # 2) check if we have a file with prof lkl values. \n",
    "#         #     * If yes, check that it has the same parameters and in the right order. Proceed. \n",
    "#         #     * If no file, start it and write the first line as param names. Proceed. \n",
    "#         #     * If file yes, but parameters don't match, then print an error. Stop. \n",
    "#         # 2) check if global minimum params have already been written (first line of file)\n",
    "#         #     * If parameters are written, check that they match global minimum. Don't write them again\n",
    "#         #     * If parameters are written but don't match, spit out error. \n",
    "#         #     * If no params written, add this current ML values for all parameters in append mode\n",
    "        \n",
    "#     param_names, param_ML, MLs = read_minimum(chains_dir, chain_file, extension='')\n",
    "\n",
    "#     global_ML = deepcopy(MLs)\n",
    "#     param_order = param_names\n",
    "\n",
    "#     try: \n",
    "#         if not match_param_names(param_names, chains_dir=chains_dir, chain_file=chain_file):\n",
    "#             raise FileExistsError\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"File not found. Starting a new file now: \" + chains_dir + chain_file + '_lkl_profile.txt \\n')\n",
    "#         with open(chains_dir + chain_file + '_lkl_profile.txt', 'w') as lkl_txt:\n",
    "#             lkl_txt.write(\"#\")\n",
    "#             for param_recorded in param_names:\n",
    "#                 lkl_txt.write(\"\\t %s\" % param_recorded)\n",
    "#             lkl_txt.write(\"\\n\")\n",
    "\n",
    "#     lkl_prof_table = np.loadtxt(chains_dir + chain_file + '_lkl_profile.txt')\n",
    "#     if lkl_prof_table.shape!=(0,):\n",
    "#         if not match_param_line(param_names, global_ML, chains_dir=chains_dir, chain_file=chain_file, loc=0):\n",
    "#             print(\"Something went wrong. The first line of the lkl_profile.txt file which should be global ML does not match the global ML in file \\n\"\n",
    "#                  +chains_dir + chain_file + '.minimum')\n",
    "#             raise FileExistsError\n",
    "#     else: \n",
    "#         write_MLs(param_names, MLs, chains_dir=chains_dir, chain_file=chain_file)\n",
    "\n",
    "\n",
    "#     ## Likelihood profile \n",
    "#         ### Set up lkl profile minimum input yaml file \n",
    "#             # Only this should be read and manipulated by the rest of the code\n",
    "\n",
    "#             # Here I check that a \"_lkl_prof\" file has been created for the minimizer input yaml.\n",
    "\n",
    "#     try:\n",
    "#         lkl_pro_yaml = yaml_load_file(chains_dir+chain_file+'_lkl_prof.minimize.input.yaml')\n",
    "#     except FileNotFoundError:\n",
    "#         run(\"cp \"+chains_dir+chain_file+'.minimize.updated.yaml'+\" \"+chains_dir+chain_file+'_lkl_prof.minimize.input.yaml', shell=True)\n",
    "#         lkl_pro_yaml = yaml_load_file(chains_dir+chain_file+'_lkl_prof.minimize.input.yaml')\n",
    "\n",
    "#             # We already have param_names and MLs saved. Update the param_ML with the values from the last entry in _lkl_prof.txt in case we're restarting / continuing a run. \n",
    "\n",
    "#     param_ML = read_lkl_output(chains_dir=chains_dir, chain_file=chain_file, loc=-1)\n",
    "#     MLs = dict(zip(param_names, param_ML))\n",
    "\n",
    "\n",
    "#     ## Run loop over increments of profile lkl param \n",
    "#         # While we are within the bounds of the profile param we want to explore: \n",
    "\n",
    "#         # 1) check if the point we are currently at i.e. param_ML and MLs, matches the last entry in the lkl_prof table.\n",
    "#         #     - if it does, the last minimum was run and saved successfully. \n",
    "#         #     - if not, check if a minimum file exists. \n",
    "#         #         - if it does, read it in and save it in the lkl prof txt. minimum run successfully. \n",
    "#         #         - if not, this happens when we have updated the yaml but the minimizer didn't finish. Run the yaml again without updating. \n",
    "#         # 2) check if minimum was run and saved. \n",
    "#         #     - if yes, update the yaml and increment the prof lkl param, update all other params to new values from current ML. Assign the MLs values for the independent params in the yaml as new reference starting points. \n",
    "#         # 3) run the minimizer \n",
    "#         # 4) save minimizer output \n",
    "    \n",
    "#     while (MLs[prof_param] < prof_max):\n",
    "#         last_entry_matches_current_params = match_param_line(param_names, MLs, chains_dir=chains_dir, chain_file=chain_file, loc=-1)\n",
    "#         if last_entry_matches_current_params:\n",
    "#             run('rm '+chains_dir + chain_file + '_lkl_prof.minimum*', shell=True)\n",
    "#             minimum_successfully_run_and_saved = True\n",
    "#         else:\n",
    "#             try:\n",
    "#                 param_names, param_ML, MLs = read_minimum(chains_dir, chain_file)\n",
    "#                 write_MLs(param_order, MLs, chains_dir=chains_dir, chain_file=chain_file)\n",
    "#                 run('rm '+chains_dir + chain_file + '_lkl_prof.minimum*', shell=True)\n",
    "#                 minimum_successfully_run_and_saved = True \n",
    "#                 print(\"-----> Minimizer run successfully for \"+prof_param+\" = \"+str(MLs[prof_param]))\n",
    "#             except OSError:\n",
    "#                 minimum_successfully_run_and_saved = False\n",
    "#                 print(\"-----> Minimizer not run for \"+prof_param+\" = \"+str(MLs[prof_param]))\n",
    "#                 print(\"       Rerunning this point\")\n",
    "\n",
    "#         if minimum_successfully_run_and_saved:\n",
    "#             increment_update_yaml(chains_dir, chain_file, MLs, lkl_pro_yaml, prof_param, prof_incr)\n",
    "#             run('rm '+chains_dir + chain_file + '_lkl_prof.minimize.updated.yaml', shell=True)\n",
    "\n",
    "#         run_minimizer(chain_file=chain_file, yaml_ext='_lkl_prof', debug=False, processes=6)\n",
    "\n",
    "#         param_names, param_ML, MLs = read_minimum(chains_dir, chain_file)\n",
    "        \n",
    "#     param_names, param_ML, MLs = read_minimum(chains_dir, chain_file)\n",
    "#     write_MLs(param_order, MLs, chains_dir=chains_dir, chain_file=chain_file)\n",
    "    \n",
    "#     lkl_profile = {}\n",
    "#     lkl_prof_table = np.loadtxt(chains_dir + chain_file + '_lkl_profile.txt')\n",
    "#     for param in param_names:\n",
    "#         lkl_profile[param] = lkl_prof_table[:,np.where(param_order==param)[0][0]]\n",
    "        \n",
    "#     return param_order, MLs, lkl_profile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
